ViT From Scratch – Research Paper Replication (ViT-Base)

This project is a complete from-scratch replication of the original Vision Transformer (ViT) architecture introduced in the paper "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale" by Dosovitskiy et al.

The implementation is done entirely in Google Colab using PyTorch, with no external ViT libraries. It focuses on rebuilding the ViT-Base configuration while deeply exploring the core architecture and theory behind it.

Key components in the notebook:

- Manual patch embedding and positional encoding

- Full implementation of Transformer Encoder blocks

- Construction of the MLP Head for classification

- Detailed explanation of the 4 key equations used in the paper

- Step-by-step breakdown of how each part maps to the original architecture

This is a research-focused deep dive, designed to help understand the mechanics of ViT at the lowest level. No inference section or pre-trained weights are included—this is purely about architecture and learning.

